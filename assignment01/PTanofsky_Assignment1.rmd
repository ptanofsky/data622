---
title: "DATA 622 Assignment 1"
subtitle: "CUNY: Spring 2021"
author: "Philip Tanofsky"
date: "`r format(Sys.time(), '%d %B %Y')`"
output: pdf_document
---

# Introduction

The purpose of this project is to apply logistic regression approaches to the Palmer Penguin data set available at https://allisonhorst.github.io/palmerpenguins/articles/intro.html. The first approach performs binary logistic regression on the dataset in order to predict a species or not of the penguin subjects. The second approach utilizes all three species to perform a multinomial logistic regression in order to predict the species of penguin subjects. While the primary goal of the logistic regression is to predict the penguin species, statistical interpretation also presents the context for the prediction models.


```{r warning=F, message=F}
# Import required R libraries
library(palmerpenguins)
library(dplyr)
library(ggplot2)
library(tidyr)
library(caret)
library(MASS)
library(pROC)
library(nnet) # Used for multinomial logistic regression
library(mlogit)
library(stargazer)
library(popbio)
# Set theme, based on the Penguin vignettes
theme_set(theme_minimal())
```

The palmer penguins dataset consists of 8 variables, 7 independent variables and 1 dependent variable (species).

## Variables

- species: species of the penguin observed (dependent variable)

- island: island of penguin's observation

- bill_length_mm: penguin bill length in millimeters

- bill_depth_mm: penguin bill depth in millimeters

- flipper_length_mm: penguin flipper length in millimeters

- body_mass_g: penguin body mass in grams

- sex: penguin sex

- year: year of observation

## EDA

Initial data summary and exploratory data analysis.

```{r warning=F,message=F}
ds <- penguins

head(ds)

summary(ds)

dim(ds)

glimpse(ds)

visdat::vis_dat(ds)
```

Initial summary outputs show 344 instances of the 8 variables. The final graph indicates the missing values among the 8 variables. Variables with missing values include $sex$, $bill\_length\_mm$, $bill\_depth\_mm$, $flipper\_length\_mm$ and $body\_mass\_g$.

```{r warning=F,message=F}
# Penguins data has three factor variables
ds %>%
  dplyr::select(where(is.factor)) %>%
  glimpse()

# Count penguins for each species / island
ds %>%
  count(species, island, .drop=F)

ggplot(ds, aes(x = island, fill = species)) +
  geom_bar(alpha = 0.8) +
  scale_fill_manual(values = c("darkorange", "purple", "cyan4"),
                    guide = F) +
  theme_minimal() +
  facet_wrap(~species, ncol = 1) +
  coord_flip()
```

The above plot indicates the population of penguin species based on island. Interesting finding that Chinstrap are only observed on Dream island and Gentoo are only observed on Biscoe island, while the Adelie species are observed on all three islands in the study.

```{r warning=F,message=F}
# Count penguins for each species / sex
ds %>%
  count(species, sex, .drop = F)

ggplot(ds, aes(x = sex, fill = species)) +
  geom_bar(alpha = 0.8) +
  scale_fill_manual(values = c("darkorange", "purple", "cyan4"),
                    guide = F) +
  theme_minimal() +
  facet_wrap(~species, ncol = 1) +
  coord_flip()
```

The above breakdown of penguin species by sex shows an expected ratio of near 50-50 for each species.

```{r warning=F,message=F}
# Penguins data also has four continuous variables, making six unique scatterplots
ds %>%
  dplyr::select(body_mass_g, ends_with("_mm")) %>%
  glimpse()

# Scatterplot example 1: penguin flipper length versus body mass
ggplot(data = penguins, aes(x = flipper_length_mm, y = body_mass_g)) +
  geom_point(aes(color = species,
                 shape = species),
             size = 2) +
  scale_color_manual(values = c("darkorange", "darkorchid", "cyan4"))
```

The above scatterplot of body mass shows a strong similarity between the species Adelie and Chinstrap with a clear difference from Gentoo. A valuable observation in regards to the binary logistic regression model.

```{r warning=F, message=F}
ds %>%
  dplyr::select(species, body_mass_g, ends_with("_mm")) %>%
  GGally::ggpairs(aes(color = species)) +
  scale_color_manual(values = c("darkorange","purple","cyan4")) +
  scale_fill_manual(values = c("darkorange","purple","cyan4"))
```

The above plot shows the additional strong similarities between the Adelie and Chinstrap species as compared to the Gentoo species.

## Model Definitions

Prep dataset for logistic regression. First step is to remove rows containing an NA.

```{r}
# Create dataset for binary logistic regression: species Gentoo or Not
data_binary <- penguins

# Only use complete instances ... actually come back to this as I don't want to exclude because of sex 11 NAs
train_data_binary <- na.omit(data_binary)

dim(train_data_binary)
```

Based on the result, 11 rows are removed, which would equal the number of NAs in variable $sex$.

# Binary Logistic Regression

The following approach attempts to construct a logistic regression model based on a binary outcome. As the penguins dataset is based on a dependent variable (species) containing three values, a dummy variable $Gentoo$ is defined to identify penguins of the species Gentoo or of the other two values (Adelie and Chinstrap). Based on the exploratory data analysis indicating independent variable overlap for body mass, bill depth, and flipper length between the Adelie and Chinstrap species, the decision was made to group these two species based on the similarities.

```{r}
# Create new column
train_data_binary$gentoo <- ifelse(train_data_binary$species=="Gentoo", 1, 0)

summary(train_data_binary)
```

With the derived dummy variable $Gentoo$, the variable $species$ is removed from the initial dataset, so as not to impact the logistic regression models.

```{r}
# Drop species column, as now just using gentoo column as Y variable

drops <- c("species")
train_data_binary <- train_data_binary[ , !(names(train_data_binary) %in% drops)]

summary(train_data_binary)
```

In order to validate the models properly, the initial penguins dataset is partitioned into training data at 70% of the given dataset with the remaining 30% used as test data completely unseen by the model.

```{r warning=F}
set.seed(123)
trainIndex <-createDataPartition(train_data_binary$gentoo, p = 0.7, list = FALSE, times = 1)
train <- train_data_binary[trainIndex,]
test <- train_data_binary[-trainIndex,]
```

Three versions of a binary logistic regression model are constructed in order to evaluate the accuracy of each and also provide to narrow the model to the least number of variables to identify the most parsimonious model.

### Baseline Model

The first model uses all the available independent variables in order to define a baseline evaluation of the model.

```{r warning=F}
# All variables
model1 <- glm(gentoo ~ ., data = train, family = "binomial"(link="logit"))
#Accuracy 100%, AIC is 18
summary(model1)
```

Resulting AIC: `r model1$aic`.

### Stepwise Model

Next, the $stepAIC$ function is applied to the full model to determine the most predictive variables for the model.

```{r warning=F}
# All variables then applied with stepAIC
model2 <- glm(gentoo ~ ., data = train, family = "binomial"(link="logit")) %>% stepAIC(trace=F, direction ='both')
# Accuracy 100% an AIC is 6
summary(model2)
```

Resulting AIC: `r model2$aic`.

### Hand Selected Model

Finally, a hand-selected list of independent variables are chosen based on the evaluation of the exploratory data analysis.

```{r warning=F}
# Hand selected variables
model3 <- glm(gentoo ~ island + bill_depth_mm + flipper_length_mm + body_mass_g, data = train, family = "binomial"(link="logit"))
# Accuracy 100%, AIC is 12
summary(model3)
```

Resulting AIC: `r model3$aic`.

### Make predictions

Predictions are performed on the test dataset based on the three above binary logistic regression models.

```{r warning=F, message=F}
## use the test data set to make predictions for the 3 models
mod1.predict.probs <- predict.glm(model1, type="response", newdata=test)
mod1.predict.manual <- ifelse(mod1.predict.probs > 0.5, '1','0')
attach(test)

mod2.predict.probs <- predict.glm(model2, type="response", newdata=test)
mod2.predict.manual <- ifelse(mod2.predict.probs > 0.5, '1','0')
attach(test)

mod3.predict.probs <- predict.glm(model3, type="response", newdata=test)
mod3.predict.manual <- ifelse(mod3.predict.probs > 0.5, '1','0')
attach(test)
```

### Model Visualizations

Plots of the data to visualize the independent variable's value compared to the logit function of the dependent variable.

First, plot the variable $flipper\_length\_mm$ against the logit value of the $Gentoo$ result.

```{r warning=F,message=F}
# Plot the dependent variable interpretation
# https://sites.google.com/site/daishizuka/toolkits/plotting-logistic-regression-in-r

# plot with flipper_length_mm on x-axis and Gentoo species (0 or 1) on y-axis
plot(flipper_length_mm,gentoo,xlab="flipper_length_mm",ylab="Probability of Gentoo")
g=glm(gentoo ~ flipper_length_mm, data = train, family = "binomial"(link="logit"))
curve(predict(g,data.frame(flipper_length_mm=x),type="resp"),add=TRUE)
```

Second, plot the variable $bill\_depth\_mm$ against the logit value of the $Gentoo$ result.

```{r warning=F,message=F}
# plot with bill_depth_mm on x-axis and Gentoo species (0 or 1) on y-axis
plot(bill_depth_mm,gentoo,xlab="bill_depth_mm",ylab="Probability of Gentoo")
g=glm(gentoo ~ bill_depth_mm, data = train, family = "binomial"(link="logit"))
curve(predict(g,data.frame(bill_depth_mm=x),type="resp"),add=TRUE)

# plot using another function
logi.hist.plot(flipper_length_mm,gentoo,boxp=FALSE,type="hist",col="gray")
```

Third plot above was just an attempt to use another library function for plotting the data against the logit function.

### Model 1 Results

The baseline model shows: 

- Accuracy: 1 or 100%

- Area Under the Curve: 1.0

- True Positive Rate (Sensitivity): 1.0

- True Negative Rate (Specificity: 1.0

- False Negative Rate (Miss Rate: 1-TPR): 0

- False Positive Rate (Fall-out: 1-TNR): 0

```{r warning=F,message=F}
# Model1
# now can use the caret function
cm.var <- caret::confusionMatrix(factor(mod1.predict.manual), factor(test$gentoo), positive='1')
cm.var$table

# print metrics
mod1.CMmetrics <- c(cm.var$overall[c(1)], cm.var$byClass[c(1,2,5,6,7)])
mod1.CMmetrics

# ROC and AUC 
par(pty="s")
roc.stepwise <- roc(train$gentoo, model1$fitted.values, plot=TRUE, print.auc=TRUE)
```

### Model 2 Results

The $stepAIC$ model shows:

- Accuracy: 1 or 100%

- Area Under the Curve: 1.0

- True Positive Rate (Sensitivity): 1.0

- True Negative Rate (Specificity: 1.0

- False Negative Rate (Miss Rate: 1-TPR): 0

- False Positive Rate (Fall-out: 1-TNR): 0

```{r warning=F,message=F}
# Model2
# now can use the caret function
cm.var <- caret::confusionMatrix(factor(mod2.predict.manual), factor(test$gentoo), positive='1')
cm.var$table

# print metrics
mod2.CMmetrics <- c(cm.var$overall[c(1)], cm.var$byClass[c(1,2,5,6,7)])
mod2.CMmetrics

# ROC and AUC 
par(pty="s")
roc.stepwise <- roc(train$gentoo, model2$fitted.values, plot=TRUE, print.auc=TRUE)
```

### Model 3 Results

The hand-selected model shows:

- Accuracy: 1 or 100%

- Area Under the Curve: 1.0

- True Positive Rate (Sensitivity): 1.0

- True Negative Rate (Specificity: 1.0

- False Negative Rate (Miss Rate: 1-TPR): 0

- False Positive Rate (Fall-out: 1-TNR): 0

```{r warning=F,message=F}
# Model3
# now can use the caret function
cm.var <- caret::confusionMatrix(factor(mod3.predict.manual), factor(test$gentoo), positive='1')
cm.var$table

# print metrics
mod3.CMmetrics <- c(cm.var$overall[c(1)], cm.var$byClass[c(1,2,5,6,7)])
mod3.CMmetrics

# ROC and AUC 
par(pty="s")
roc.stepwise <- roc(train$gentoo, model3$fitted.values, plot=TRUE, print.auc=TRUE)
```

### Variable Interpretation

For Model 3 above, the variable interpretations are as follows.

A one-unit increase in the variable $island$ for value $Dream$ is associated with the decrease in the log odds of being in species Gentoo in the amount of 1.39.

A one-unit increase in the variable $island$ for value $Torgersen$ is associated with the decrease in the log odds of being in species Gentoo in the amount of 5.044.

A one-unit increase in the variable $bill\_depth\_mm$ is associated with the decrease in the log odds of being in species Gentoo in the amount of 10.49.

A one-unit increase in the variable $flipper\_length\_mm$ is associated with the increase in the log odds of being in species Gentoo in the amount of 1.098.

A one-unit increase in the variable $body\_mass\_g$ is associated with the increase in the log odds of being in species Gentoo in the amount of .01958.

## Alternate Binary Model

Looking at the summary results for the three models attempting to identify Gentoo species or not, I decided to make the goal a bit more difficult as Adelie species appears on all three islands, and statistically, the Adelie species does overlap with Chinstrap, I decided to create logistic regression models to identify Adelie instead of Gentoo.

The first model uses the same baseline model approach as above including all independent variables. The second model re-uses the $stepAIC$ approach to algorithmically select the best independent variables.

```{r warning=F,message=F}
# Create dataset for binary logistic regression: species Adelie or Not
data_binary <- penguins

# Only use complete instances ... actually come back to this as I don't want to exclude because of sex 11 NAs
train_data_binary <- na.omit(data_binary)

train_data_binary$adelie <- ifelse(train_data_binary$species=="Adelie", 1, 0)

summary(train_data_binary)

drops <- c("species")
train_data_binary <- train_data_binary[ , !(names(train_data_binary) %in% drops)]

set.seed(123)
trainIndex <-createDataPartition(train_data_binary$adelie, p = 0.7, list = FALSE, times = 1)
train <- train_data_binary[trainIndex,]
test <- train_data_binary[-trainIndex,]

# All variables
model1_ad <- glm(adelie ~ ., data = train, family = "binomial"(link="logit"))
summary(model1_ad)

# All variables then applied with stepAIC
model2_ad <- glm(adelie ~ ., data = train, family = "binomial"(link="logit")) %>% stepAIC(trace=F, direction ='both')
summary(model2_ad)

## use the test data set to make predictions for the 3 models
mod1_ad.predict.probs <- predict.glm(model1_ad, type="response", newdata=test)
mod1_ad.predict.manual <- ifelse(mod1_ad.predict.probs > 0.5, '1','0')
attach(test)

mod2_ad.predict.probs <- predict.glm(model2_ad, type="response", newdata=test)
mod2_ad.predict.manual <- ifelse(mod2_ad.predict.probs > 0.5, '1','0')
attach(test)

# Model1
# now can use the caret function
cm.var <- caret::confusionMatrix(factor(mod1_ad.predict.manual), factor(test$adelie), positive='1')
cm.var$table

# print metrics
mod1_ad.CMmetrics <- c(cm.var$overall[c(1)], cm.var$byClass[c(1,2,5,6,7)])
mod1_ad.CMmetrics

# ROC and AUC 
par(pty="s")
roc.stepwise <- roc(train$adelie, model1_ad$fitted.values, plot=TRUE, print.auc=TRUE)
```


The baseline model shows:

- Accuracy: 0.9797980 or ~98%

- Area Under the Curve: 1.0

- True Positive Rate (Sensitivity): 0.9534884

- True Negative Rate (Specificity: 1.0

- False Negative Rate (Miss Rate: 1-TPR): 0.0465116

- False Positive Rate (Fall-out: 1-TNR): 0

```{r warning=F,message=F}
# Model2
# now can use the caret function
cm.var <- caret::confusionMatrix(factor(mod2_ad.predict.manual), factor(test$adelie), positive='1')
cm.var$table

# print metrics
mod2_ad.CMmetrics <- c(cm.var$overall[c(1)], cm.var$byClass[c(1,2,5,6,7)])
mod2_ad.CMmetrics

# ROC and AUC 
par(pty="s")
roc.stepwise <- roc(train$adelie, model2_ad$fitted.values, plot=TRUE, print.auc=TRUE)
```

The $stepAIC$ model shows:

- Accuracy: 0.9797980 or ~98%

- Area Under the Curve: 1.0

- True Positive Rate (Sensitivity): 0.9534884

- True Negative Rate (Specificity: 1.0

- False Negative Rate (Miss Rate: 1-TPR): 0.0465116

- False Positive Rate (Fall-out: 1-TNR): 0

Interestingly, the $stepAIC$ did not perform at 100% accuracy. The $stepAIC$ model indicates $bill\_length\_mm$, $bill\_depth\_mm$, and $body\_mass\_g$ are the three most predictive independent variables.

# Multinomial Logistic Regression

The following approach attempts to construct a multinomial logistic regression model based on a multivariate outcome. As the penguins dataset is based on a dependent variable (species) containing three values, these models attempt to predict the species of each penguin subject. 

## Model 1

The baseline model is attempted for the multinomial logistic regression to predict the penguin species.

```{r warning=F,message=F}
# using https://www.r-bloggers.com/2020/05/multinomial-logistic-regression-with-r/

mlr_data <- penguins

mlr_data <- na.omit(mlr_data)

index <- createDataPartition(mlr_data$species, p = .70, list = FALSE)
train <- mlr_data[index,]
test <- mlr_data[-index,]

# Set the reference 
train$species <- relevel(train$species, ref = "Adelie")

# Training the multinomial model
multinom_model1 <- multinom(species ~ ., data = mlr_data)

# Checking the model
summary(multinom_model1)
```

The final negative log-likelihood value of 0.000004 is produced by running the model. This value multiplied by two is then seen in the model summary as the Residual Deviance.

```{r warning=F,message=F}
stargazer(multinom_model1, type="text", out="multinom_model1.htm")
```

The $stargazer$ produces a clear table to view the coefficients of the independent variables in the model.

```{r warning=F,message=F}
z <- summary(multinom_model1)$coefficients/summary(multinom_model1)$standard.errors
(z)

p <- (1 - pnorm(abs(z), 0, 1)) * 2
(p)
```

The above results of the 2-tailed z test show variables $island$, $bill\_length\_mm$, and $bill\_depth\_mm$ play no role in the prediction of the species for baseline model.

```{r warning=F,message=F}
# Convert the coefficients to odds by taking the exponential of the coefficients.
exp(coef(multinom_model1))

head(pp <- fitted(multinom_model1))
```

The above results indicate the species odds for each penguin observed.

```{r warning=F}
# Predicting and validating the model

# Predicting the values for train dataset
train$speciesPredicted <- predict(multinom_model1, newdata = train, "class")

# Building classification table
tab <- table(train$species, train$speciesPredicted)

# Calculating accuracy - sum of diagonal elements divided by total obs
round((sum(diag(tab))/sum(tab))*100,2)
```

The accuracy of the predictions against the initial training dataset is 100%.

```{r warning=F,message=F}
# Predicting the class for test dataset
test$speciesPredicted <- predict(multinom_model1, newdata = test, "class")

# Building classification table
tab <- table(test$species, test$speciesPredicted)
tab
```

The output table for the predictions of the test dataset show 100% accuracy, also.

## Model 2

Given the results of the baseline model and desire to great a parsimonious model, the below model uses independent variables selected based on value to the model.

```{r warning=F,message=F}
index <- createDataPartition(mlr_data$species, p = .70, list = FALSE)
train <- mlr_data[index,]
test <- mlr_data[-index,]

# Set the reference 
train$species <- relevel(train$species, ref = "Adelie")

# Training the multinomial model
multinom_model2 <- multinom(species ~ island + bill_depth_mm + bill_length_mm, data = mlr_data)

# Checking the model
summary(multinom_model2)
```

The final negative log-likelihood value of 0.073060 is produced by running the model. This value multiplied by two (0.1461206) is then seen in the model summary as the Residual Deviance.

### Variable Interpretation

- A one-unit increase in the variable bill_depth_mm is associated with the decrease in the log odds of being a Chinstrap species vs. Adelie species in the amount of 16.84742.

- A one-unit increase in the variable bill_depth_mm is associated with the decrease in the log odds of being a Gentoo species vs. Adelie species in the amount of 24.91635.

- A one-unit increase in the variable bill_length_mm is associated with the increase in the log odds of being a Chinstrap species vs. Adelie species in the amount of 9.933821.

- A one-unit increase in the variable bill_length_mm is associated with the increase in the log odds of being a Gentoo species vs. Adelie species in the amount of 10.469682.

- The log odds of being a Chinstrap species vs. Adelie species will increase by 1.288634 if moving from island Biscoe to island Dream.

- The log odds of being a Chinstrap species vs. Adelie species will decrease by -33.22371 if moving from island Biscoe to island Torgersen.

- The log odds of being a Gentoo species vs. Adelie species will decrease by 20.848914 if moving from island Biscoe to island Dream.

- The log odds of being a Gentoo species vs. Adelie species will decrease by 18.53702 if moving from island Biscoe to island Torgersen.

```{r warning=F,message=F}
z <- summary(multinom_model2)$coefficients/summary(multinom_model2)$standard.errors
(z)

# 2-tailed z test
p <- (1 - pnorm(abs(z), 0, 1)) * 2
(p)
```

The above results of the 2-tailed z test show each selected independent variable plays a factor in the prediction of the species for the second model.

```{r warning=F,message=F}
# Convert the coefficients to odds by taking the exponential of the coefficients.
exp(coef(multinom_model2))

head(pp <- fitted(multinom_model2))
```

Again, the first six rows are displayed to show the probabilities of the species for each penguin observed.

```{r warning=F,message=F}
# Predicting and validating the model

# Predicting the values for train dataset
train$speciesPredicted <- predict(multinom_model2, newdata = train, "class")

# Building classification table
tab <- table(train$species, train$speciesPredicted)

# Calculating accuracy - sum of diagonal elements divided by total obs
round((sum(diag(tab))/sum(tab))*100,2)
```

As in the baseline model, the accuracy of the predictions against the initial training dataset is 100%.

```{r warning=F,message=F}
# Predicting the class for test dataset
test$speciesPredicted <- predict(multinom_model2, newdata = test, "class")

# Building classification table
tab <- table(test$species, test$speciesPredicted)
tab
```

And fortunately, the output table for the predictions of the test dataset show 100% accuracy, also. As this model relies on just three independent variables ($island$, $bill\_depth\_mm$, and $bill\_length\_mm$), the hand-selected model proves to be the best parsimonious model.

### Attempt at Extra Credit

After some Internet searching, it appears there isn't much direction in how to measure model fit for multinomial logistic regression models. Pearson residual and overdispersion are ways to measure the model. Approaches to comparing two or more models would include likelihood ratio test, Wald test, cross validation, and parallel lines assumption.

# Conclusion

Overall, the simplicity of the palmer penguins dataset allowed for the creation of highly accurate binary and multinomial logistic regression models. Even with the purposely more difficult binary prediction, the model still performed well. The variable interpretation provided gives a clear picture of the weights by each selected variable impacts the model. Given the lack of clear fit evaluation for the multinomial model, I do think further analysis is warranted to ensure the model fit is reasonable.
