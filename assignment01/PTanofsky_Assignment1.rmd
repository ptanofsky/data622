---
title: "DATA 622 Assignment 1"
subtitle: "CUNY: Spring 2021"
author: "Philip Tanofsky"
date: "`r format(Sys.time(), '%d %B %Y')`"
output: pdf_document
---

```{r warning=F, message=F}
library(palmerpenguins)
library(dplyr)
library(ggplot2)
library(tidyr)
library(caret)
library(MASS)
library(pROC)
library(nnet) # Used for multinomial logistic regression
library(mlogit)
library(stargazer)
library(popbio)
theme_set(theme_minimal())
```

The palmer penguins dataset consists of 8 variables, 7 independent variables and 1 dependent variable (species).

## Variables
species: species of the penguin observed
island: consider it (No NA)
bill_length_mm: penguin bill length in millimeters
bill_depth_mm: penguin bill depth in millimeters
flipper_length_mm: penguin flipper length in millimeters
body_mass_g: penguin body mass in grams
sex: penguin sex
year: year of observation

```{r}
ds <- penguins

head(ds)

summary(ds)

dim(ds)

glimpse(ds)

visdat::vis_dat(ds)

# Penguins data has three factor variables
ds %>%
  dplyr::select(where(is.factor)) %>%
  glimpse()

# Count penguins for each species / island
ds %>%
  count(species, island, .drop=F)

ggplot(ds, aes(x = island, fill = species)) +
  geom_bar(alpha = 0.8) +
  scale_fill_manual(values = c("darkorange", "purple", "cyan4"),
                    guide = F) +
  theme_minimal() +
  facet_wrap(~species, ncol = 1) +
  coord_flip()

# Count penguins for each species / sex
ds %>%
  count(species, sex, .drop = F)

ggplot(ds, aes(x = sex, fill = species)) +
  geom_bar(alpha = 0.8) +
  scale_fill_manual(values = c("darkorange", "purple", "cyan4"),
                    guide = F) +
  theme_minimal() +
  facet_wrap(~species, ncol = 1) +
  coord_flip()

# Penguins data also has four continuous variables, making six unique scatterplots
ds %>%
  dplyr::select(body_mass_g, ends_with("_mm")) %>%
  glimpse()

# Scatterplot example 1: penguin flipper length versus body mass
ggplot(data = penguins, aes(x = flipper_length_mm, y = body_mass_g)) +
  geom_point(aes(color = species,
                 shape = species),
             size = 2) +
  scale_color_manual(values = c("darkorange", "darkorchid", "cyan4"))
```

```{r warning=F, message=F}
ds %>%
  dplyr::select(species, body_mass_g, ends_with("_mm")) %>%
  GGally::ggpairs(aes(color = species)) +
  scale_color_manual(values = c("darkorange","purple","cyan4")) +
  scale_fill_manual(values = c("darkorange","purple","cyan4"))
```

```{r}
# Create dataset for binary logistic regression: species Gentoo or Not
data_binary <- penguins

# Only use complete instances ... actually come back to this as I don't want to exclude because of sex 11 NAs
train_data_binary <- na.omit(data_binary)

dim(train_data_binary)
```

Based on the result, 11 rows are removed, which would equal the number of NAs in variable $sex$.

# Binary Logistic Regression

The following approach attempts to construct a logistic regression model based on a binary outcome. As the penguins dataset is based on a dependent variable (species) containing three values, a dummy variable $Gentoo$ is defined to identify penguins of the species Gentoo or of the other two values (Adelie and Chinstrap). Based on the exploratory data analysis indicating independent variable overlap for body mass, bill depth, and flipper length between the Adelie and Chinstrap species, the decision was made to group these two species based on the similarities.

```{r}
# Create new column
train_data_binary$gentoo <- ifelse(train_data_binary$species=="Gentoo", 1, 0)

summary(train_data_binary)
```

With the derived dummy variable $Gentoo$, the variable $species$ is removed from the initial dataset, so as not to impact the logistic regression models.

```{r}
# Drop species column, as now just using gentoo column as Y variable

drops <- c("species")
train_data_binary <- train_data_binary[ , !(names(train_data_binary) %in% drops)]

summary(train_data_binary)
```

In order to validate the models property, the initial penguins dataset is partitioned into training data at 70% of the given dataset with the remaining 30% used as test data completely unseen by the model.

```{r warning=F}
set.seed(123)
trainIndex <-createDataPartition(train_data_binary$gentoo, p = 0.7, list = FALSE, times = 1)
train <- train_data_binary[trainIndex,]
test <- train_data_binary[-trainIndex,]
```

Three versions of a binary logistic regression model are constructed in order to evaluate the accuracy of each and also provide to narrow the model to the least number of variables to identify the most parsimomious model.

### Baseline Model

The first model uses all the available independent variables in order to define a baseline evaluation of the model.

```{r warning=F}

# All variables
model1 <- glm(gentoo ~ ., data = train, family = "binomial"(link="logit"))
#Accuracy 100%, AIC is 18
summary(model1)
```

Resulting AIC: `r model1$aic`.

### Stepwise Model

Next, the $stepAIC$ function is applied to the full model to determine the most ??meaningful?? variables for the model.

```{r warning=F}
# All variables then applied with stepAIC
model2 <- glm(gentoo ~ ., data = train, family = "binomial"(link="logit")) %>% stepAIC(trace=F, direction ='both')
# Accuracy 100% an AIC is 6
summary(model2)
```

Resulting AIC: `r model2$aic`.

### Hand Selected Model

Finally, a hand-selected list of independent variables are selected based on the evaluation of the exploratory data analysis.

```{r warning=F}
# Hand selected variables
model3 <- glm(gentoo ~ island + bill_depth_mm + flipper_length_mm + body_mass_g, data = train, family = "binomial"(link="logit"))
# Accuracy 100%, AIC is 12
summary(model3)
```

Resulting AIC: `r model3$aic`.

```{r warning=F}
## use the test data set to make predictions for the 3 models
mod1.predict.probs <- predict.glm(model1, type="response", newdata=test)
mod1.predict.manual <- ifelse(mod1.predict.probs > 0.5, '1','0')
attach(test)

mod2.predict.probs <- predict.glm(model2, type="response", newdata=test)
mod2.predict.manual <- ifelse(mod2.predict.probs > 0.5, '1','0')
attach(test)

mod3.predict.probs <- predict.glm(model3, type="response", newdata=test)
mod3.predict.manual <- ifelse(mod3.predict.probs > 0.5, '1','0')
attach(test)
```

```{r warning=F}
# Plot the dependent variable interpretation
# https://sites.google.com/site/daishizuka/toolkits/plotting-logistic-regression-in-r

# plot with flipper_length_mm on x-axis and Gentoo species (0 or 1) on y-axis
plot(flipper_length_mm,gentoo,xlab="flipper_length_mm",ylab="Probability of Gentoo")
g=glm(gentoo ~ flipper_length_mm, data = train, family = "binomial"(link="logit"))
curve(predict(g,data.frame(flipper_length_mm=x),type="resp"),add=TRUE)

# plot with bill_depth_mm on x-axis and Gentoo species (0 or 1) on y-axis
plot(bill_depth_mm,gentoo,xlab="bill_depth_mm",ylab="Probability of Gentoo")
g=glm(gentoo ~ bill_depth_mm, data = train, family = "binomial"(link="logit"))
curve(predict(g,data.frame(bill_depth_mm=x),type="resp"),add=TRUE)

# plot using another function
logi.hist.plot(flipper_length_mm,gentoo,boxp=FALSE,type="hist",col="gray")
```

### Model 1 Results

```{r warning=F}
# Model1
# now can use the caret function
cm.var <- caret::confusionMatrix(factor(mod1.predict.manual), factor(test$gentoo), positive='1')
cm.var$table

# print metrics
mod1.CMmetrics <- c(cm.var$overall[c(1)], cm.var$byClass[c(1,2,5,6,7)])
mod1.CMmetrics

# ROC and AUC 
par(pty="s")
roc.stepwise <- roc(train$gentoo, model1$fitted.values, plot=TRUE, print.auc=TRUE)

# Dispersion Statistic
E2 <- resid(model1, type = "pearson")
N  <- nrow(train)
p  <- length(coef(model1)) + 1 # '+1' is due to theta
mod1.dispersion <- dispesion <-sum(E2^2) / (N - p)
```

### Model 2 Results

```{r warning=F}
# Model2
# now can use the caret function
cm.var <- caret::confusionMatrix(factor(mod2.predict.manual), factor(test$gentoo), positive='1')
cm.var$table

# print metrics
mod2.CMmetrics <- c(cm.var$overall[c(1)], cm.var$byClass[c(1,2,5,6,7)])
mod2.CMmetrics

# ROC and AUC 
par(pty="s")
roc.stepwise <- roc(train$gentoo, model2$fitted.values, plot=TRUE, print.auc=TRUE)

# Dispersion Statistic
E2 <- resid(model2, type = "pearson")
N  <- nrow(train)
p  <- length(coef(model2)) + 1 # '+1' is due to theta
mod2.dispersion <- dispesion <-sum(E2^2) / (N - p)
```

### Model 3 Results

```{r warning=F}
# Model3
# now can use the caret function
cm.var <- caret::confusionMatrix(factor(mod3.predict.manual), factor(test$gentoo), positive='1')
cm.var$table

# print metrics
mod3.CMmetrics <- c(cm.var$overall[c(1)], cm.var$byClass[c(1,2,5,6,7)])
mod3.CMmetrics

# ROC and AUC 
par(pty="s")
roc.stepwise <- roc(train$gentoo, model3$fitted.values, plot=TRUE, print.auc=TRUE)

# Dispersion Statistic
E2 <- resid(model3, type = "pearson")
N  <- nrow(train)
p  <- length(coef(model3)) + 1 # '+1' is due to theta
mod3.dispersion <- dispesion <-sum(E2^2) / (N - p)
```

# Multinomial Logistic Regression

```{r warning=F}
# Initial walk-through: https://stats.idre.ucla.edu/r/dae/multinomial-logistic-regression/

# Start with initial dateset
mlr_data <- penguins

summary(mlr_data)

mlr_data$species2 <- relevel(mlr_data$species, ref = "Gentoo")
test <- multinom(species2 ~ body_mass_g + bill_length_mm + bill_depth_mm + flipper_length_mm + island, data = mlr_data)

summary(test)

stargazer(test, type="text", out="test.htm")

test.rrr = exp(coef(test))
test.rrr

stargazer(test, type="text", coef=list(test.rrr), p.auto=FALSE, out="testrrr.htm")
```

```{r warning=F}
# Again with https://www.r-bloggers.com/2020/05/multinomial-logistic-regression-with-r/

index <- createDataPartition(mlr_data$species, p = .70, list = FALSE)
train <- mlr_data[index,]
test <- mlr_data[-index,]

# Set the reference 
train$species <- relevel(train$species, ref = "Adelie")

# Training the multinomial model
#multinom_model <- multinom(species ~ ., data = mlr_data)

multinom_model <- multinom(species ~ island + bill_depth_mm + bill_length_mm, data = mlr_data)

#multinom_model <- multinom(species ~ flipper_length_mm + body_mass_g, data = mlr_data)


#with flipper length, and with body mass, will also get to one hundred accuracy
# but these 3 are required for 100: island + bill_depth_mm + bill_length_mm

# Checking the model
summary(multinom_model)

# Convert the coefficients to odds by taking the exponential of the coefficients.
exp(coef(multinom_model))

head(round(fitted(multinom_model), 2))

```

```{r warning=F}
# Predicting and validating the model

# Predicting the values for train dataset
train$speciesPredicted <- predict(multinom_model, newdata = train, "class")

# Building classification table
tab <- table(train$species, train$speciesPredicted)

# Calculating accuracy - sum of diagonal elements divided by total obs
round((sum(diag(tab))/sum(tab))*100,2)

# Predicting the class for test dataset
test$speciesPredicted <- predict(multinom_model, newdata = test, "class")

# Building classification table
tab <- table(test$species, test$speciesPredicted)
tab
```



Sex doesn't matter, 
Considering that I'm grouping Orange and Purple, probably don't use bill_length_mm, as that one shows purple and green have similar distribution




# Prompt

Data – 622
Homework # 1
Due date Feb 19, 2021- 11:59 EST
Let’s use the Penguin dataset for our assignment. To learn more about the dataset, please visit:
https://allisonhorst.github.io/palmerpenguins/articles/intro.html
For this assignment, let us use ‘species’ as our outcome or the dependent variable.
1. Logistic Regression with a binary outcome. (40)
a. The penguin dataset has ‘species’ column. Please check how many categories
you have in the species column. Conduct whatever data manipulation you need to do to be able to build a logistic regression with binary outcome. Please explain your reasoning behind your decision as you manipulate the outcome/dependent variable (species).
b. Please make sure you are evaluating the independent variables appropriately in deciding which ones should be in the model.
c. Provide variable interpretations in your model.
2. For your model from #1, please provide: AUC, Accuracy, TPR, FPR, TNR, FNR (20)
3. Multinomial Logistic Regression. (40)
a. Please fit it a multinomial logistic regression where your outcome variable is
‘species’.
b. Please be sure to evaluate the independent variables appropriately to fit your
best parsimonious model.
c. Please be sure to interpret your variables in the model.
4. Extra credit: what would be some of the fit statistics you would want to evaluate for your model in question #3? Feel free to share whatever you can provide. (10)
 
 
 Considers for multinomial
 wald test
 LR test: likelihood ratio
 Cross validation
 parallel lines assumption
 
 Ideas here: https://stats.stackexchange.com/questions/145203/how-to-assess-if-a-model-is-good-in-multinomial-logistic-regression