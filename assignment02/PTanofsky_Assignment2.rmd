---
title: "DATA 622 Assignment 2"
subtitle: "CUNY: Spring 2021"
author: "Philip Tanofsky"
date: "`r format(Sys.time(), '%d %B %Y')`"
output: pdf_document
---

# Introduction

The purpose of this project is to apply generative model approaches to the Palmer Penguin data set available at https://allisonhorst.github.io/palmerpenguins/articles/intro.html. The first approach performs linear discriminant analysis (LDA) on the dataset in order to predict the species of the penguin subjects. The second approach performs a quadratic discriminant analysis in order to predict the species of penguin subjects. The final approach uses the Naive Bayes modeling approach to also predict the species of the penguin subjects. The exploratory data analysis informs the decisions of the predictor variables included for each generative model. A final comparison of the models are presented in order to compare the accuracy of each.

```{r warning=F, message=F}
# Import required R libraries
library(palmerpenguins)
library(tidyverse)
library(caret)
library(MASS)
library(ggplot2)
library(mvtnorm)
library(e1071)
library(klaR)
theme_set(theme_classic())
```

# Initial Data Inspection

```{r warning=F,message=F}
ds <- penguins

head(ds)

summary(ds)

dim(ds)
```

The palmer penguins dataset consists of 8 variables, 7 independent variables and 1 dependent variable (species).

## Variables

- species: species of the penguin observed (dependent variable)

- island: island of penguin's observation

- bill_length_mm: penguin bill length in millimeters

- bill_depth_mm: penguin bill depth in millimeters

- flipper_length_mm: penguin flipper length in millimeters

- body_mass_g: penguin body mass in grams

- sex: penguin sex

- year: year of observation

# Exploratory Data Analysis

For linear discriminant analysis, two assumptions are made about the data. One, the predictors are normally distributed, which means the data follows a Gaussian distribution for each class. Two, the classes have class-specific means but also have equal variance and covariance.

First step, confirm multivariate normal distribution. A density plot of the four continuous variables by species indicates the normal distribution for the class-specific plots.

```{r warning=F,message=F}
# Overlayed density plots
featurePlot(x = penguins[, 3:6],
            y = penguins$species,
            plot = "density",
            # Pass in options to xyplot() to
            # make it prettier
            scales = list(x = list(relation="free"),
                          y = list(relation="free")),
            adjust = 1.5,
            pch = "|",
            layout = c(2, 2),
            auto.key = list(columns = 3))
```

The variable $flipper_length_mm$ shows the clearest example of normal distribution by species class. The four density plots also indicate common evaluations between the Adelie and Chinstrap species for the independent variables flipper_length_mm, body_mass_g, and bill_depth_mm. The remaining continuous variable, bill_length_mm, shows an overlap between the Chinstrap and Gentoo species.

The scatterplot matrix of the continuous variables indicates the relationships between the independent variables for each of the three penguin species.

```{r warning=F,message=F}
# Use featurePlot
# https://topepo.github.io/caret/visualizations.html

# Scatterplot
featurePlot(x = penguins[, 3:6],
            y = penguins$species,
            plot = "pairs",
            # Add a key at the top
            auto.key = list(columns = 3))
```

The independent variable $bill_length_mm$ stands out by providing a clear distinction between the three penguin species when compared with any of the other three independent variables. As seen in the above scatterplot, when using any two of the other three independent variables, the plot results in a clear overlap of the Adelie and Chinstrap species, an expected result given the prior density plots.

So far, the independent variable $bill_length_mm$ is a prime candidate to be included in the generative models. The key will be identifying which of the other variables will provide additional significance to the model.

The following boxplots provide another perspective of the continuous variable relationships across the three species. As already noted, Adelie and Chinstrap show similar means and distributions for three of the four variables. The boxplot does indicate a few outliers across the variable values but nothing egregious.

```{r warning=F,message=F}
featurePlot(x = penguins[, 3:6], 
            y = penguins$species, 
            plot = "box", 
            ## Pass in options to bwplot() 
            scales = list(y = list(relation="free"),
                          x = list(rot = 90)),  
            layout = c(2,2), 
            auto.key = list(columns = 2))
```

With the first assumption confirmed, the second assumption is to confirm the similar covariance across the species classes. The following covariance checks further identify the relationship between the variables.

```{r warning=F,message=F}
# Covariance matrix
p_g <- penguins %>% filter(species == 'Gentoo')
cov_mat <- cov(p_g[,3:6],  use = "complete.obs")
round(cov_mat, 2)

p_a <- penguins %>% filter(species == 'Adelie')
cov_mat <- cov(p_a[,3:6],  use = "complete.obs")
round(cov_mat, 2)

p_c <- penguins %>% filter(species == 'Chinstrap')
cov_mat <- cov(p_c[,3:6],  use = "complete.obs")
round(cov_mat, 2)
```

Comparing the variance of each continuous independent variable across the three species classes, $body\_mass\_g$ shows the divergence in the variance evaluation, particularly for the Gentoo species. Then again, this variable is a weight measured in grams, so perhaps dividing each value by 1000 to measure in kilograms would make the variance seems less divergent. The prior density plot does show the wider distribution among Gentoo species for $body\_mass\_g$.

The covariance, in which a low number indicates a weak relationship, $bill\_length\_mm$ and $bill\_depth\_mm$. Two variables, $bill\_depth\_mm$ and $flipper\_length\_mm$, also show a comparatively low covariance. $bill\_length\_mm$ and $flipper\_length\_mm$ indicates a covariance higher than the aforementioned pairs, but the resulting values across the species classes are much better than the not listed variable pairs.

Assessing the mean of each continuous independent variable for each species class, the variable $bill\_depth\_mm$ highlights the least difference in mean values of four variables. In fact, the difference in mean for Adelie and Chinstrap is one tenth of the measurement.

```{r warning=F,message=F}
# Means
p_a[,3:6] %>% summarise_each(funs( mean( .,na.rm = TRUE)))
p_g[,3:6] %>% summarise_each(funs( mean( .,na.rm = TRUE)))
p_c[,3:6] %>% summarise_each(funs( mean( .,na.rm = TRUE)))
```

## Correlation matrix

Based on the covariance matrix, the correlation matrix should confirm the variable relationships as previously noted. Again, $bill\_length\_mm$ and $bill\_depth\_mm$ show the least correlation. $body\_mass\_g$ and $flipper\_length\_mm$ have a high correlation.

```{r warning=F,message=F}
# Compute correlation matrix
cor_mat <- cor(penguins[,3:6],  use = "complete.obs")
round(cor_mat, 2)

library(corrplot)
corrplot(cor_mat, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45)

```

## Categorical Variables

For comprehensive exploratory data analysis, the categorical independent variables are plotted and assessed.

The breakdown of penguin species by island indicates an uneven distribution by island. Gentoo only appear on Biscoe island, Chinstrap only appear on Dream, and Adelie appears on all three island under consideration, Torgensen, Dream and Biscoe.

```{r warning=F,message=F}
# Count penguins for each species / island
ggplot(ds, aes(x = island, fill = species)) +
  geom_bar(alpha = 0.8) +
  scale_fill_manual(values = c("darkorange", "purple", "cyan4"),
                    guide = F) +
  theme_minimal() +
  facet_wrap(~species, ncol = 1) +
  coord_flip()
```

The penguin category for sex shows a clear even distribution by species with a few values missing for Adelie and Gentoo species.

```{r warning=F,message=F}
# Count penguins for each species / sex
ggplot(ds, aes(x = sex, fill = species)) +
  geom_bar(alpha = 0.8) +
  scale_fill_manual(values = c("darkorange", "purple", "cyan4"),
                    guide = F) +
  theme_minimal() +
  facet_wrap(~species, ncol = 1) +
  coord_flip()
```

The penguin category for year also indicates a relatively even distribution for each species.

```{r warning=F,message=F}
# Count penguins for each species / year
ggplot(ds, aes(x = year, fill = species)) +
  geom_bar(alpha = 0.8) +
  scale_fill_manual(values = c("darkorange", "purple", "cyan4"),
                    guide = F) +
  theme_minimal() +
  facet_wrap(~species, ncol = 1) +
  coord_flip()
```

As an additional evaluation of the independent variables, a MANOVA test is applies of the four continuous variables along with year, which is defined as numeric. The categorical variables are not applicable for the MANOVA test. The results show a high significance for the four continuous variables while also indicating the year variable is not significant. These results confirm previous understanding of the independent variables.

```{r warning=F,message=F}
# MANOVA test
manova_res <- manova(cbind(bill_length_mm,bill_depth_mm,flipper_length_mm,body_mass_g,year) ~ species, data = penguins)
summary.aov(manova_res)
```

\newpage

# Linear Discriminant Analysis (LDA)

Linear discriminant analysis is a classification method that separates classes through linear directions, or linear discriminants. The linear directions are derived as linear combinations of the predictor variables. As outlined above, LDA assumes the feature variables come from multivariate normal distributions and each one continuous. The different classes should also have class-specific means and equal variance/covariance. LDA does not handle categorical data well.

To prepare the data for the LDA model, the observations with missing data are removed from the dataset. Next, the categorical variables sex and year are removed. Based on the exploratory data analysis, neither variable appears to distinguish one species from another. Despite LDA not handling categorical data well, I chose to leave the variable island in the dataset as a trial.

In preparation of evaluating the model's accuracy, the penguins dataset is split into training and set partitions. Finally, the training and test datasets are transformed through a preprocessing step to normalize the data. Given the different range and distribution of the independent variable values, particularly the $body\_mass\_g$ variable, the normalization ensures each variable will be weighted based on predictive ability and not based on initial measurement value.

```{r warning=F,message=F,eval=T}
# Load the data
data("penguins")

# Only complete entries
penguins <- na.omit(penguins)

# Remove 'year' and 'sex' feature
# Apparently leaving 'island' in for LDA improves the model
drops <- c("year", "sex")
penguins <- penguins[ , !(names(penguins) %in% drops)]

#Split the data into training (75%) and test set (25%)
set.seed(123)
training.samples <- penguins$species %>%
  createDataPartition(p = 0.75, list=FALSE)
train.data <- penguins[training.samples, ]
test.data <- penguins[-training.samples, ]

#2. Normalize the data. Categorial variables are automatically ignored from normalizing
# Estimate preprocessing parameters
preproc.param <- train.data %>%
  preProcess(method = c("center", "scale"))

# Transform the data using the estimated parameters
train.transformed <- preproc.param %>% predict(train.data)
test.transformed <- preproc.param %>% predict(test.data)

```

Based on the exploratory data analysis, the three variables selected for the LDA model (and the two subsequent models) are $bill\_length\_mm$, $flipper\_length\_mm$ and $bill\_depth\_mm$. As the $body\_mass\_g$ variable was highly correlated with $bill\_length\_mm$ and $flipper\_length\_mm$, the variable was omitted from the model. The categorical variables were also omitted, as LDA performs best on continuous variables.

```{r warning=F,message=F,eval=T}
# Fit the model
model <- lda(species~bill_length_mm + flipper_length_mm + bill_depth_mm, data = train.transformed)

# Output Model
model
```

When there are K classes, linear discriminant analysis can be viewed exactly in a K-1 dimensional plot.
With three classes in the species dependent variables, two linear discriminants are generated.

The plot of the LD1 and LD2 values on the two-dimensional plot shows the classification of the species in the 2D subspace.

```{r warning=F,message=F,eval=T}
# Plot
lda.data <- cbind(train.transformed, predict(model)$x)
ggplot(lda.data, aes(LD1, LD2)) +
  geom_point(aes(color = species))
```

Now predict the species on the transformed test dataset.

```{r warning=F,message=F,eval=T}
# Make predictions
predictions <- model %>% predict(test.transformed)

head(predictions$x, 3)

# Confusion matrix
table(predictions$class, test.transformed$species)

# Model accuracy
mean(predictions$class == test.transformed$species)
```

The confusion matrix indicates one incorrect prediction with an overall model accuracy of 98.78%.

The 2D subspace plot of LD1 and LD2 on the test dataset show a similar pattern to the scatterplot on the training data. The plot does show one outlier Chinstrap observation.

```{r warning=F,message=F,eval=T}

# Plot
lda.data <- cbind(test.transformed, predictions$x)
ggplot(lda.data, aes(LD1, LD2)) +
  geom_point(aes(color = species))

ldahist(data=predictions$x[,1], g=test.transformed$species)

ldahist(data=predictions$x[,2], g=test.transformed$species)
```

The bar plot for LD1 shows a better performance in classifying among the three species, while the LD2 bar plot shows more overlap among the three species. These plots align with the proportion of trace from the initial model output. LD1 accounts for over 88% of the trace, while LD2 accounts for less than 12%.

The partition plots below from the $partimat$ function display the classification of each test observation for each combination of two independent variables. Turns out the plot with the lowest error rate is $bill\_length\_mm$ and $body\_mass\_g$.

```{r warning=F,message=F,eval=T}
partimat(species ~ bill_length_mm + bill_depth_mm + flipper_length_mm + body_mass_g, data=test.transformed, method="lda")
```

\newpage

# Quadratic Discriminant Analysis (QDA)

Quadratic discriminant analysis, similar to linear discriminant analysis, also assumes a multivariate Gaussian distribution, but unlike LDA, QDA assumes each class has a unique covariance matrix. QDA draws the classification distinctions through quadratic decision boundaries, as the name implies, instead of the linear approach of LDA. To allow for even comparison among the three models, the same transformed training dataset is used for the QDA model.

```{r warning=F,message=F,eval=T}
# QDA
# Fit the model
model <- qda(species~bill_length_mm + flipper_length_mm + bill_depth_mm, data = train.transformed)

# Output model results
model
```

WORDS_HERE

```{r warning=F,message=F,eval=T}
# Make predictions
predictions <- model %>% predict(test.transformed)

# Confusion matrix
table(predictions$class, test.transformed$species)

# Model accuracy
mean(predictions$class == test.transformed$species)
```

The confusion matrix indicates two incorrect predictions with an overall model accuracy of 97.56%. Based on the textbook description of QDA, LDA typically outperforms QDA when relatively fewer training observations are used. Also, as the covariance matrix outlined in the exploratory data analysis, the assumptions are met for the LDA model approach. That being said, the difference in accuracy is based on a single additional incorrect prediction.

The partition plots below capture the quadratic nature of the decision boundaries.

```{r warning=F,message=F,eval=T}
partimat(species ~ bill_length_mm + bill_depth_mm + flipper_length_mm + body_mass_g, data=test.transformed, method="qda")
```

\newpage

# Naive Bayes

Assumes all predictor variables are independent. in the case of numeric features, NB makes a strong assumption that the numerical variable is normally distributed.

WORDS_HERE

```{r warning=F,message=F,eval=T}
# Modified for the penguin data

# Fitting the Naive Bayes model
nbm <- naiveBayes(species~bill_length_mm + flipper_length_mm + bill_depth_mm, data=train.transformed)

# Output the model
nbm
```

WORDS_HERE

```{r warning=F,message=F,eval=T}
# Prediction on the dataset
nb_predictions <- predict(nbm, test.transformed)
# Confusion matrix to check accuracy
table(nb_predictions, test.transformed$species)

mean(nb_predictions == test.transformed$species)
```

The confusion matrix indicates one incorrect prediction with an overall model accuracy of 98.78%. 

WORDS_HERE

# Conclusion

ROC curves

WORDS_HERE

Palmer Penguins citation

# ==== Prompt =====

```{r eval=F}
Homework # 2 (Generative Models) (100 points) Due on March 12, 11:59pm EST
We will be working with the Penguin dataset again as we did for Homework #1. Please use "Species" as your target variable. For this assignment, you may want to drop/ignore the variable "year".
Using the target variable, Species, please conduct:
a. LinearDiscriminantAnalysis(30points):
a. Youwanttoevaluateallthe'features'ordependentvariablesandsee what should be in your model. Please comment on your choices.
b. Justasuggestion:YoumightwanttoconsiderexploringfeaturePlot on the caret package. Basically, you look at each of the features/dependent variables and see how they are different based on species. Simply eye-balling this might give you an idea about which would be strong 'classifiers' (aka predictors).
c. Fit your LDA model using whatever predictor variables you deem appropriate. Feel free to split the data into training and test sets before fitting the model.
d. Lookatthefitstatistics/accuracyrates.
b. QuadraticDiscriminantAnalysis(30points)
a. Samestepsasabovetoconsider
c. Naive Bayes (30 points)
a. Samestepsasabovetoconsider
d. Commentonthemodelsfits/strength/weakness/accuracyforallthesethree models that you worked with. (10 points)
```


http://www.sthda.com/english/articles/36-classification-methods-essentials/146-discriminant-analysis-essentials-in-r/

Make sure each variable is normally distributed

https://web.stanford.edu/class/stats202/notes/Classification/LDA.html
That is, within each class the features have multivariate normal distribution with center depending on the class and common covariance

In the covariance matrix in the output, the off-diagonal elements contain the covariances of each pair of variables. The diagonal elements of the covariance matrix contain the variances of each variable. The variance measures how much the data are scattered about the mean.

bill_depth, bill_length, body_mass

# From: https://rpubs.com/Nolan/298913
plot(model, dimen = 1, type = "b")

https://www.geeksforgeeks.org/linear-discriminant-analysis-in-r-programming/

https://www.geeksforgeeks.org/naive-bayes-classifier-in-r-programming/

https://www.r-bloggers.com/2018/01/understanding-naive-bayes-classifier-using-r/

Focus on normal distributions

If n is small and the distribution of the predictors X is approximately normal in each of the classes, the LD model is more stable than logistic regression

Correlation will cause a line in the Gaussian density plot

