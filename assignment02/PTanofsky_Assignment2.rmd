---
title: "DATA 622 Assignment 2"
subtitle: "CUNY: Spring 2021"
author: "Philip Tanofsky"
date: "`r format(Sys.time(), '%d %B %Y')`"
output: pdf_document
---

# Introduction

The purpose of this project is to apply generative model approaches to the Palmer Penguin data set available at https://allisonhorst.github.io/palmerpenguins/articles/intro.html. The first approach performs linear discriminant analysis (LDA) on the dataset in order to predict the species of the penguin subjects. The second approach performs a quadratic discriminant analysis in order to predict the species of penguin subjects. The final approach uses the Naive Bayes modeling approach to also predict the species of the penguin subjects. The exploratory data analysis informs the decisions of the predictor variables included for each generative model. A final comparison of the models are presented in order to compare the accuracy of each.

```{r warning=F, message=F}
# Import required R libraries
library(palmerpenguins)
library(tidyverse)
library(caret)
library(MASS)
library(ggplot2)
library(mvtnorm)
library(e1071)
library(klaR)
theme_set(theme_classic())
```

# Initial Data Inspection

```{r warning=F,message=F}
ds <- penguins

head(ds)

summary(ds)

dim(ds)
```

The palmer penguins dataset consists of 8 variables, 7 independent variables and 1 dependent variable (species).

## Variables

- species: species of the penguin observed (dependent variable)

- island: island of penguin's observation

- bill_length_mm: penguin bill length in millimeters

- bill_depth_mm: penguin bill depth in millimeters

- flipper_length_mm: penguin flipper length in millimeters

- body_mass_g: penguin body mass in grams

- sex: penguin sex

- year: year of observation

# Exploratory Data Analysis

For linear discriminant analysis, two assumptions are made about the data. One, the predictors are normally distributed, which means the data follows a Gaussian distribution for each class. Two, the classes have class-specific means but also have equal variance and covariance.

First step, confirm multivariate normal distribution. A density plot of the four continuous variables by species indicates the normal distribution for the class-specific plots.

```{r warning=F,message=F}
# Overlayed density plots
featurePlot(x = penguins[, 3:6],
            y = penguins$species,
            plot = "density",
            # Pass in options to xyplot() to
            # make it prettier
            scales = list(x = list(relation="free"),
                          y = list(relation="free")),
            adjust = 1.5,
            pch = "|",
            layout = c(2, 2),
            auto.key = list(columns = 3))
```

The variable $flipper_length_mm$ shows the clearest example of normal distribution by species class. The four density plots also indicate common evaluations between the Adelie and Chinstrap species for the independent variables flipper_length_mm, body_mass_g, and bill_depth_mm. The remaining continuous variable, bill_length_mm, shows an overlap between the Chinstrap and Gentoo species.

The scatterplot matrix of the continuous variables indicates the relationships between the independent variables for each of the three penguin species.

```{r warning=F,message=F}
# Use featurePlot
# https://topepo.github.io/caret/visualizations.html

# Scatterplot
featurePlot(x = penguins[, 3:6],
            y = penguins$species,
            plot = "pairs",
            # Add a key at the top
            auto.key = list(columns = 3))
```

The independent variable $bill_length_mm$ stands out by providing a clear distinction between the three penguin species when compared with any of the other three independent variables. As seen in the above scatterplot, when using any two of the other three independent variables, the plot results in a clear overlap of the Adelie and Chinstrap species, an expected result given the prior density plots.

So far, the independent variable $bill_length_mm$ is a prime candidate to be included in the generative models. The key will be identifying which of the other variables will provide additional significance to the model.

The following boxplots provide another perspective of the continuous variable relationships across the three species. As already noted, Adelie and Chinstratp show similar means and distributions for three of the four variables. The boxplot does indicate a few outlies across the variable values but nothing egregious.

```{r warning=F,message=F}
featurePlot(x = penguins[, 3:6], 
            y = penguins$species, 
            plot = "box", 
            ## Pass in options to bwplot() 
            scales = list(y = list(relation="free"),
                          x = list(rot = 90)),  
            layout = c(2,2), 
            auto.key = list(columns = 2))
```

With the first assumption confirmed, the second assumption is to confirm the similar covariance across the species classes. The following covariance checks further identify the relationship between the variables.

```{r warning=F,message=F}
# Covariance matrix
p_g <- penguins %>% filter(species == 'Gentoo')
cov_mat <- cov(p_g[,3:6],  use = "complete.obs")
round(cov_mat, 2)

p_a <- penguins %>% filter(species == 'Adelie')
cov_mat <- cov(p_a[,3:6],  use = "complete.obs")
round(cov_mat, 2)

p_c <- penguins %>% filter(species == 'Chinstrap')
cov_mat <- cov(p_c[,3:6],  use = "complete.obs")
round(cov_mat, 2)
```

Comparing the variance of each continuous independent variable across the three species classes, $body\_mass\_g$ shows the divergence in the variance evaluation, particularly for the Gentoo species. Then again, this variable is a weight measured in grams, so perhaps dividing each value by 1000 to measure in kilograms would make the variance seems less divergent. The prior density plot does show the wider distribution among Gentoo species for $body\_mass\_g$.

The covariance, in which a low number indicates a weak relationship, $bill\_length\_mm$ and $bill\_depth\_mm$. Two variables, $bill\_depth\_mm$ and $flipper\_length\_mm$, also show a comparatively low covariance. $bill\_length\_mm$ and $flipper\_length\_mm$ indicates a covariance higher than the aforementioned pairs, but the resulting values across the species classes are much better than the not listed variable pairs.

Assessing the mean of each continuous independent variable for each species class, the variable $bill\_depth\_mm$ highlights the least difference in mean values of four variables. In fact, the difference in mean for Adelie and Chinstrap is one tenth of the measurement.

```{r warning=F,message=F}
# Means
p_a[,3:6] %>% summarise_each(funs( mean( .,na.rm = TRUE)))
p_g[,3:6] %>% summarise_each(funs( mean( .,na.rm = TRUE)))
p_c[,3:6] %>% summarise_each(funs( mean( .,na.rm = TRUE)))
```

## Correlation matrix

Based on the covariance matrix, the correlation matrix should confirm the variable relationships as previously noted. Again, $bill\_length\_mm$ and $bill\_depth\_mm$ show the least correlation. $body\_mass\_g$ and $flipper\_length\_mm$ have a high correlation.

```{r warning=F,message=F}
# Compute correlation matrix
cor_mat <- cor(penguins[,3:6],  use = "complete.obs")
round(cor_mat, 2)

library(corrplot)
corrplot(cor_mat, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45)

```

## Categorical Variables

For comprehensive exploratory data analysis, the categorical independent variables are plotted and assessed.

The breakdown of penguin species by island indicates an uneven distribution by island. Gentoo only appear on Biscoe island, Chinstrap only appear on Dream, and Adelie appears on all three island under consideration, Torgensen, Dream and Bisco.

```{r warning=F,message=F}
# Count penguins for each species / island
ggplot(ds, aes(x = island, fill = species)) +
  geom_bar(alpha = 0.8) +
  scale_fill_manual(values = c("darkorange", "purple", "cyan4"),
                    guide = F) +
  theme_minimal() +
  facet_wrap(~species, ncol = 1) +
  coord_flip()
```

The penguin category for sex shows a clear even distribution by species with a few values missing for Adelie and Gentoo species.

```{r warning=F,message=F}
# Count penguins for each species / sex
ggplot(ds, aes(x = sex, fill = species)) +
  geom_bar(alpha = 0.8) +
  scale_fill_manual(values = c("darkorange", "purple", "cyan4"),
                    guide = F) +
  theme_minimal() +
  facet_wrap(~species, ncol = 1) +
  coord_flip()
```

The penguin category for year also indicates a relatively even distribution for each species.

```{r warning=F,message=F}
# Count penguins for each species / year
ggplot(ds, aes(x = year, fill = species)) +
  geom_bar(alpha = 0.8) +
  scale_fill_manual(values = c("darkorange", "purple", "cyan4"),
                    guide = F) +
  theme_minimal() +
  facet_wrap(~species, ncol = 1) +
  coord_flip()
```

As an additional evaluation of the independent variables, a MANOVA test is applies of the four continuous variables along with year, which is defined as numeric. The categorial variables are not applicable for the MANOVA test. The results show a high significance for the four continuous variables while also indicating the year variable is not significant. These results confirm previous understanding of the independent variables.

```{r warning=F,message=F}
# MANOVA test
manova_res <- manova(cbind(bill_length_mm,bill_depth_mm,flipper_length_mm,body_mass_g,year) ~ species, data = penguins)
summary.aov(manova_res)
```

\newpage

# Linear Discriminant Analysis (LDA)

Linear discriminant analysis is a classification method that separates classes through linear directions, or linear discrimants. The linear directions are derived as linear combinations of the predictor variables. As outlined above, LDA assumes the feature variables come from multivariate normal distributions and each one continuous. The different classes should also have class-specific means and equal variance/covariance. LDA does not handle categorical data well.

To prepare the data for the LDA model, the observations with missing data are removed from the dataset. Next, the categorical variables sex and year are removed. Based on the exploratory data analysis, neither variable appears to distinguish one species from another. Despite LDA not handling categorical data well, I chose to leave the variable island in the dataset as a trial.

In preparation of evaluating the model's accuracy, the penguins dataset is split into training and set partitions. Finally, the training and test datasets are transformed through a preprocessing step to normalize the data. Given the different range and distribution of the independent variable values, particularly the $body\_mass\_g$ variable, the normalization ensures each variable will be weighted based on predictive ability and not based on initial measurement value.

```{r warning=F,message=F,eval=T}
# Load the data
data("penguins")

# Only complete entries
penguins <- na.omit(penguins)

# Remove 'year' and 'sex' feature
# Apparently leaving 'island' in for LDA improves the model
drops <- c("year", "sex")
penguins <- penguins[ , !(names(penguins) %in% drops)]

#Split the data into training (75%) and test set (25%)
set.seed(123)
training.samples <- penguins$species %>%
  createDataPartition(p = 0.75, list=FALSE)
train.data <- penguins[training.samples, ]
test.data <- penguins[-training.samples, ]

#2. Normalize the data. Categorial variables are automatically ignored from normalizing
# Estimate preprocessing parameters
preproc.param <- train.data %>%
  preProcess(method = c("center", "scale"))

# Transform the data using the estimated parameters
train.transformed <- preproc.param %>% predict(train.data)
test.transformed <- preproc.param %>% predict(test.data)

```


Focus on normal distributions

If n is small and the distribution of the predictors X is approximately normal in each of the classes, the LD model is more stable than logistic regression

Correlation will cause a line in the Gaussian density plot

When there are K classes, linear discriminant analysis can be viewed exactly in a K-1 dimensional plot.
Measuring which centroid is the closest. Distance in the subspace

```{r warning=F,message=F,eval=T}
# Fit the model
model <- lda(species~bill_length_mm + flipper_length_mm + bill_depth_mm, data = train.transformed)

# Output Model
model
```

```{r warning=F,message=F,eval=T}
# Make predictions
predictions <- model %>% predict(test.transformed)

head(predictions$x, 3)

# Confusion matrix
table(predictions$class, test.transformed$species)

# Model accuracy
mean(predictions$class == test.transformed$species)
```

```{r warning=F,message=F,eval=T}
# Plot
lda.data <- cbind(train.transformed, predict(model)$x)
ggplot(lda.data, aes(LD1, LD2)) +
  geom_point(aes(color = species))
```

```{r warning=F,message=F,eval=T}
ldahist(data=predictions$x[,1], g=test.transformed$species)

ldahist(data=predictions$x[,2], g=test.transformed$species)
```

```{r warning=F,message=F,eval=T}
partimat(species ~ bill_length_mm + bill_depth_mm + flipper_length_mm + body_mass_g, data=train.transformed, method="lda")
```

\newpage

# Quadratic Discriminant Analysis

QDA: works well with fewer features, that's when NB works well, works with higher number of features

mixed features can be used for NB

```{r warning=F,message=F,eval=T}
# QDA

# Remove 'island' feature as it was causing rank deficiency in group Chinstrap
#drops <- c("flipper_length_mm", "body_mass_g", "island")
drops <- c("island")
train.transformed <- train.transformed[ , !(names(train.transformed) %in% drops)]

# Fit the model
model <- qda(species~bill_length_mm + flipper_length_mm + bill_depth_mm, data = train.transformed)

# Output model results
model
```

```{r warning=F,message=F,eval=T}
# Make predictions
predictions <- model %>% predict(test.transformed)

# Confusion matrix
table(predictions$class, test.transformed$species)

# Model accuracy
mean(predictions$class == test.transformed$species)

partimat(species ~ bill_length_mm + bill_depth_mm + flipper_length_mm + body_mass_g, data=train.transformed, method="qda")
```

\newpage

# Naive Bayes


```{r warning=F,message=F,eval=T}
# Modified for the penguin data

# Fitting the Naive Bayes model
nbm <- naiveBayes(species~bill_length_mm + flipper_length_mm + bill_depth_mm, data=train.transformed)

# Output the model
nbm

# Prediction on the dataset
nb_predictions <- predict(nbm, test.transformed)
# Confusion matrix to check accuracy
table(nb_predictions, test.transformed$species)

mean(nb_predictions == test.transformed$species)
```

CONSIDER REMOVING THIS ONE BELOW, ALL I NEED IS A SINGLE NAIVE BAYES, NOT TWO

```{r warning=F,message=F,eval=T}
# Getting started with Naive Bayes in mlr
library(mlr)

# Create a classification task for learning on the Dataset and specify the target feature
task <- makeClassifTask(data = train.transformed, target="species")

# Initialize the Naive Bayes classifier
selected_model <- makeLearner("classif.naiveBayes")

# Train the model
nb_mlr <- train(selected_model, task)

# Read the model learned
nb_mlr$learner.model

# Predict on the dataset without passing the target feature
predictions_mlr <- as.data.frame(predict(nb_mlr, newdata = test.transformed))

# Confusion matrix to check accuracy
table(predictions_mlr[,1], test.transformed$species)

mean(predictions_mlr[,1] == test.transformed$species)
```

# Conclusion

ROC curves


Palmer Penguins citation

# ==== Prompt =====

```{r eval=F}
Homework # 2 (Generative Models) (100 points) Due on March 12, 11:59pm EST
We will be working with the Penguin dataset again as we did for Homework #1. Please use "Species" as your target variable. For this assignment, you may want to drop/ignore the variable "year".
Using the target variable, Species, please conduct:
a. LinearDiscriminantAnalysis(30points):
a. Youwanttoevaluateallthe'features'ordependentvariablesandsee what should be in your model. Please comment on your choices.
b. Justasuggestion:YoumightwanttoconsiderexploringfeaturePlot on the caret package. Basically, you look at each of the features/dependent variables and see how they are different based on species. Simply eye-balling this might give you an idea about which would be strong 'classifiers' (aka predictors).
c. Fit your LDA model using whatever predictor variables you deem appropriate. Feel free to split the data into training and test sets before fitting the model.
d. Lookatthefitstatistics/accuracyrates.
b. QuadraticDiscriminantAnalysis(30points)
a. Samestepsasabovetoconsider
c. Naive Bayes (30 points)
a. Samestepsasabovetoconsider
d. Commentonthemodelsfits/strength/weakness/accuracyforallthesethree models that you worked with. (10 points)
```


http://www.sthda.com/english/articles/36-classification-methods-essentials/146-discriminant-analysis-essentials-in-r/

Make sure each variable is normally distributed

https://web.stanford.edu/class/stats202/notes/Classification/LDA.html
That is, within each class the features have multivariate normal distribution with center depending on the class and common covariance

In the covariance matrix in the output, the off-diagonal elements contain the covariances of each pair of variables. The diagonal elements of the covariance matrix contain the variances of each variable. The variance measures how much the data are scattered about the mean.

bill_depth, bill_length, body_mass

# From: https://rpubs.com/Nolan/298913
plot(model, dimen = 1, type = "b")

https://www.geeksforgeeks.org/linear-discriminant-analysis-in-r-programming/

https://www.geeksforgeeks.org/naive-bayes-classifier-in-r-programming/

https://www.r-bloggers.com/2018/01/understanding-naive-bayes-classifier-using-r/