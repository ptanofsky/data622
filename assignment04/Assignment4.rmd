---
title: "DATA 622 Assignment 4"
author: "Philip Tanofsky"
date: "4/23/2021"
output: html_document
---

```{r warning=F, message=F}
# Import required R libraries
#library(vcd)
library(kernlab)
library(caret)
#library(MASS)
#library(ggplot2)
#library(mvtnorm)
#library(e1071)
#library(klaR)
#library(pROC)
#library(corrplot)
theme_set(theme_classic())

library(tidyverse)
library(tidymodels)
library(skimr)
#library(baguette)
#library(future)
#library(xgboost)
library(vip)
library(rpart.plot)
```

```{r warning=F, message=F}
# Read in loan approval csv
data <- read_csv("https://raw.githubusercontent.com/completegraph/DATA622_MACHINELEARNING/main/HW4/ADHD_data.csv")
names(data) <- make.names(names(data),unique = TRUE)

# Remove ADHD individual questions
data <- data[, -c(5:22)]

# Remove MD individual questions
data <- data[, -c(6:20)]

# Remove Psych.meds (too many NAs)
data <- data[, -c(21)]

# Remove instances with missing data
data <- na.omit(data)

#data <- data %>% 
#  mutate_if(is.numeric, as.factor)

data$Suicide <- as.factor(data$Suicide)

# Display skim summary
skim(data)
```


http://www.rebeccabarter.com/blog/2020-03-25_machine_learning/
```{r warning=F, message=F}
# Split into training and test data
set.seed(1234)
# split the data into training (75%) and testing (25%)
data_split <- initial_split(data, prop = .8)
data_split

data_train <- training(data_split)
data_test <- testing(data_split)
```

```{r warning=F, message=F}
# Create CV object from training data
data_cv <- vfold_cv(data_train, v=5, repeats=5)
```

```{r warning=F, message=F, eval=F}
# Recipe attempt v0 ... ignore for now
# Define the recipe
data_recipe <- recipe(Suicide ~ THC + Cocaine + Opioids, data = data) %>%
  step_normalize(all_numeric()) %>%
  step_knnimpute(all_predictors())
```

Age + Sex + Race + ADHD.Total + MD.TOTAL + Alcohol + THC + Cocaine + Stimulants + Sedative.hypnotics + Opioids + Court.order + Education + Hx.of.Violence + Disorderly.Conduct + Abuse + Non.subst.Dx + Subst.Dx

```{r warning=F, message=F}
# https://www.tidymodels.org/learn/work/tune-svm/
data_recipe <- recipe(Suicide ~ Sex + ADHD.Total + MD.TOTAL + Abuse, data = data) %>%
  step_normalize(all_numeric(), -all_outcomes()) %>%
  step_dummy(all_nominal(), -all_outcomes())  %>%
  # Remove any zero variance predictors
  step_zv(all_predictors()) %>%
  # Remove any linear combinations
  step_lincomb(all_numeric())
```

From Alex: predictors for suicide
ADHD, Mood, Abuse and seem to impact women more than men.

All predictors:
Age + Sex + Race + ADHD.Total + MD.TOTAL + Alcohol + THC + Cocaine + Stimulants + Sedative.hypnotics + Opioids + Court.order + Education + Hx.of.Violence + Disorderly.Conduct + Abuse + Non.subst.Dx + Subst.Dx

```{r warning=F, message=F}
set.seed(1234)
data_rs <- bootstraps(data, times = 30)
```

```{r warning=F, message=F}
# Consider log-likelihood statistic
# using ROC curve for now
roc_vals <- metric_set(roc_auc, accuracy, kap)
```

```{r warning=F, message=F}
# Verbose turned off, save out-of-sample predictions is turned on
ctrl <- control_grid(verbose=F, save_pred=T)
```


```{r warning=F, message=F}
# SVM Model with Polynomial
svm_model <- svm_poly(cost = tune(),
                      degree = tune()) %>% 
  set_engine("kernlab") %>% 
  set_mode("classification") %>% 
  translate()
```

```{r warning=F, message=F, eval=F}
svm_model <- svm_rbf(cost = tune(),
                     rbf_sigma = tune()) %>%
  set_mode("classification") %>%
  set_engine("kernlab")
```

```{r warning=F, message=F}
# Generate formula
formula_res <- svm_model %>%
  tune_grid(data_recipe,
    resamples = data_cv,
    metrics = roc_vals,
    control = ctrl
  )

formula_res
```

Suicide ~ Age + Sex + Race + ADHD.Total + MD.TOTAL + Alcohol + THC + Cocaine + Stimulants + Sedative.hypnotics + Opioids + Court.order + Education + Hx.of.Violence + Disorderly.Conduct + Abuse + Non.subst.Dx + Subst.Dx

```{r warning=F, message=F}
# The .metrics column contains tibbles of the performance metrics for each tuning parameter combination
formula_res %>%
  select(.metrics) %>%
  slice(1) %>%
  pull(1)
```

```{r warning=F, message=F}
# Get final resampling estimates
estimates <- collect_metrics(formula_res)
estimates
```

```{r warning=F, message=F}
# Display top combinations
show_best(formula_res, metric = "roc_auc")
```

```{r warning=F, message=F}
# Execute with a recipe
set.seed(1234)
recipe_res <-
  svm_model %>%
  tune_grid(
    data_recipe,
    resamples = data_cv,
    metrics = roc_vals,
    control = ctrl
  )
recipe_res
```

```{r warning=F, message=F}
# Best setting
show_best(recipe_res, metric = "roc_auc")
```

```{r warning=F, message=F}
# As save_pred = TRUE, we have kept the out-of-sample predictions for each sample
collect_predictions(recipe_res)
```

```{r warning=F, message=F}
# Set the workflow
svm_wf <- workflow() %>%
  add_recipe(data_recipe) %>%
  add_model(svm_model)
```

```{r warning=F, message=F, eval=F}
# Tune parameters
svm_grid <- expand.grid(mtry = c(3,4,5))

svm_tune_results <- svm_wf %>%
  tune_grid(resamples = data_cv,
            grid = svm_grid,
            metrics = metric_set(accuracy, roc_auc, kap)
            )

# Print results
svm_tune_results %>%
  collect_metrics()
```

```{r warning=F, message=F}
# Select best model based on roc_auc
best_svm <- recipe_res %>%
  select_best(metric = 'roc_auc')

# view the best svm parameters
best_svm
```

```{r warning=F,message=F}
# finalize workflow
final_svm_wf <- svm_wf %>%
  finalize_workflow(best_svm)
```

```{r warning=F,message=F}
# fit the model
svm_wf_fit <- final_svm_wf %>%
  fit(data = data_train)
```

```{r warning=F,message=F}
svm_fit <- svm_wf_fit %>%
  pull_workflow_fit()

# Received error model-specific variable importance scores are currently not available for this type of model
#vip(svm_fit)
```

```{r warning=F,message=F}
# Received error: Not an rpart object
#rpart.plot(svm_fit$fit, roundint=FALSE)
```

```{r warning=F,message=F}
# train and evaluate
svm_last_fit <- final_svm_wf %>%
  last_fit(data_split)

svm_last_fit %>% collect_metrics()
```


```{r warning=F,message=F}
#svm_last_fit %>% collect_predictions() %>%
#  roc_curve(truth = Suicide, estimate = .pred_N) %>%
#  autoplot()

svm_predictions <- svm_last_fit %>% collect_predictions()

conf_mat(svm_predictions, truth = Suicide, estimate = .pred_class)
```

```{r warning=F, message=F, eval=F}
svm_fit <- svm_wf %>%
  last_fit(data_split)

svm_fit
```

```{r warning=F, message=F, eval=F}
test_perf <- svm_fit %>% collect_metrics()
test_perf
```

```{r warning=F, message=F, eval=F}
test_predictions <- svm_fit %>% collect_predictions()
test_predictions
```


```{r warning=F, message=F, eval=F}
test_predictions %>%
  conf_mat(truth = Suicide, estimate = .pred_class)
```

```{r warning=F, message=F, eval=F}
test_predictions %>%
  ggplot() +
  geom_density(aes(x = .pred_pos, fill = data),
               alpha = 0.5)
```

Results

With svm_poly, no tuning
##           Truth
## Prediction  0  1
##          0 20 11
##          1  1  3

Results

with svm_rbf, with tuning
##           Truth
## Prediction  0  1
##          0 21 14
##          1  0  0

With svm_poly, with tuning
##           Truth
## Prediction  0  1
##          0 21 13
##          1  0  1

With svm_poly, with tuning
##           Truth
## Prediction  0  1
##          0 18 11
##          1  2  4